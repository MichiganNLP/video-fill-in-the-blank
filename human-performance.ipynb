{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "QuestionNumber = 5  # question per page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to organize answers\n",
    "\n",
    "def arrange_answers(answers):    # organize answers in one instance\n",
    "    arranged_answers =[[] for _ in range(QuestionNumber)]\n",
    "    for name in answers.keys():\n",
    "        answer = answers[name].strip().lower()    # remove sapce & convert to lower\n",
    "        for i in range(QuestionNumber):\n",
    "            if \"answer\"+str(i+1)+\"-\" in name:     # belong to question i\n",
    "                arranged_answers[i].append(answer)\n",
    "    return [sorted(set(t), key=t.index) for t in  arranged_answers]  # remove repeated answer # keep the answer order\n",
    "\n",
    "\n",
    "def answer_frequency(Answers):          # count the frequency of each answer\n",
    "    ritem = dict()\n",
    "    for ans in Answers:\n",
    "        if ans in ritem: ritem[ans] += 1\n",
    "        else: ritem[ans] = 1\n",
    "    return ritem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to calculate metrics\n",
    "\n",
    "def human_metric(anss,StandAnswer=None,RemoveZero=False):\n",
    "    '''\n",
    "    Calculate metrics for answers of one question\n",
    "    Input:\n",
    "    anss: All answers for one question\n",
    "    StandAnswer: Standard answer; \n",
    "        If 'None', only take the most frequent word as the \"mode\"\n",
    "    RemoveZero: Using RemoveZero method or not\n",
    "    '''\n",
    "    if len(anss) == 1:          # only one answer for this question\n",
    "        if len(anss[0]) != 0:   # this one answer is not empty\n",
    "            return [[1] for _ in range(25)]\n",
    "        else:                   # this one answer is empty\n",
    "            return [[0] for _ in range(25)]\n",
    "    # Met1, Met2, VQA1, VQA2, VQA3\n",
    "    # the best,the first,the average of first 2,the average of first 3,the average of all\n",
    "    Mets = np.zeros((len(anss),5,5))\n",
    "    ResWorkerNumber = len(anss)-1\n",
    "    for ind in range(len(anss)):\n",
    "        ans = anss[ind]\n",
    "        if len(anss[ind]) == 0:\n",
    "            for mx in range(5):\n",
    "                for my in range(5):\n",
    "                    Mets[ind,mx,my] = 0\n",
    "        else:                   # answer not empty\n",
    "            Otherans = [anss[i] for i in range(len(anss)) if i != ind]\n",
    "            OtherAns = [ite for eachans in Otherans for ite in eachans]\n",
    "            AnsCountDic = answer_frequency(OtherAns)\n",
    "            # find mode\n",
    "            modes = [sorted(AnsCountDic.items(),key=lambda item:item[1])[-1][0]]\n",
    "            ModeFreq = AnsCountDic[modes[0]]\n",
    "            if StandAnswer is not None:  # ground truth is also a mode\n",
    "                modes.append(StandAnswer)\n",
    "            metrics = []\n",
    "            metrics.append([AnsCountDic[ite]/ModeFreq if ite in AnsCountDic \n",
    "                        else 0 for ite in ans])  # Met1\n",
    "            metrics.append([1 if ite in modes else \n",
    "                            AnsCountDic[ite]/ResWorkerNumber if ite in AnsCountDic \n",
    "                            else 0 for ite in ans])  # Met2\n",
    "            metrics.append([min(AnsCountDic[ite],1) if ite in AnsCountDic \n",
    "                            else 0 for ite in ans])  # VQA1\n",
    "            metrics.append([min(AnsCountDic[ite]/2.0,1) if ite in AnsCountDic \n",
    "                            else 0 for ite in ans])  # VQA2\n",
    "            metrics.append([min(AnsCountDic[ite]/3.0,1) if ite in AnsCountDic \n",
    "                            else 0 for ite in ans])  # VAQ3\n",
    "            # the best,the first,the average of first 2,the average of first 3,the average of all\n",
    "            for mi in range(5):\n",
    "                Mets[ind,mi,0] = max(metrics[mi])\n",
    "                Mets[ind,mi,1] = metrics[mi][0]\n",
    "                Mets[ind,mi,2] = np.average(metrics[mi][:2]) if len(metrics[mi]) >=2 else np.average(metrics[mi])\n",
    "                Mets[ind,mi,3] = np.average(metrics[mi][:3]) if len(metrics[mi]) >=3 else np.average(metrics[mi])\n",
    "                Mets[ind,mi,4] = np.average(metrics[mi])\n",
    "    if RemoveZero:          # remove the zero score answers in the first round\n",
    "        remain = [ite != 0 for ite in Mets[:,0,0]]  # mark whether to remain the answers\n",
    "        Mets = np.zeros((len(anss),5,5))\n",
    "        for ind in range(len(anss)):\n",
    "            ans = anss[ind]\n",
    "            if len(ans)==0 or remain[ind]==0:\n",
    "                for mx in range(5):\n",
    "                    for my in range(5):\n",
    "                        Mets[ind,mx,my] = 0\n",
    "            else:                   # answer not empty\n",
    "                Otherans = [anss[i] for i in range(len(anss)) if (i!=ind and remain[i]!=0)]\n",
    "                OtherAns = [ite for eachans in Otherans for ite in eachans]\n",
    "                resworkernumber = len(Otherans)\n",
    "                if resworkernumber != 0:\n",
    "                    AnsCountDic = answer_frequency(OtherAns)\n",
    "                    # find mode\n",
    "                    modes = [sorted(AnsCountDic.items(),key=lambda item:item[1])[-1][0]]\n",
    "                    ModeFreq = AnsCountDic[modes[0]]\n",
    "                    if StandAnswer is not None:  # ground truth is also a mode\n",
    "                        modes.append(StandAnswer)                    \n",
    "                    metrics = []\n",
    "                    metrics.append([AnsCountDic[ite]/ModeFreq if ite in AnsCountDic \n",
    "                                else 0 for ite in ans])  # Met1\n",
    "                    metrics.append([1 if ite in modes else \n",
    "                                    AnsCountDic[ite]/resworkernumber if ite in AnsCountDic \n",
    "                                    else 0 for ite in ans])  # Met2\n",
    "                    metrics.append([min(AnsCountDic[ite],1) if ite in AnsCountDic \n",
    "                                    else 0 for ite in ans])  # VQA1\n",
    "                    metrics.append([min(AnsCountDic[ite]/2.0,1) if ite in AnsCountDic \n",
    "                                    else 0 for ite in ans])  # VQA2\n",
    "                    metrics.append([min(AnsCountDic[ite]/3.0,1) if ite in AnsCountDic \n",
    "                                    else 0 for ite in ans])  # VAQ3\n",
    "                    # the best,the first,the average of first 2,the average of first 3,the average of all\n",
    "                    for mi in range(5):\n",
    "                        Mets[ind,mi,0] = max(metrics[mi])\n",
    "                        Mets[ind,mi,1] = metrics[mi][0]\n",
    "                        Mets[ind,mi,2] = np.average(metrics[mi][:2]) if len(metrics[mi]) >=2 else np.average(metrics[mi])\n",
    "                        Mets[ind,mi,3] = np.average(metrics[mi][:3]) if len(metrics[mi]) >=3 else np.average(metrics[mi])\n",
    "                        Mets[ind,mi,4] = np.average(metrics[mi])\n",
    "                else:     # No other answers left \n",
    "                    for mx in range(5):\n",
    "                        for my in range(5):\n",
    "                            Mets[ind,mx,my] = 1\n",
    "    return np.reshape(Mets,(len(anss),-1)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch1_result = pd.read_csv(\"Batch_4058534_batch_results.csv\")\n",
    "batch2_result = pd.read_csv(\"Batch_4090592_batch_results.csv\")\n",
    "original_result = pd.concat([batch1_result,batch2_result],axis=0)\n",
    "val = pd.read_csv(\"Validation.csv\")\n",
    "test = pd.read_csv(\"Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_infos = dict() # hit: workers' id, video id, question\n",
    "\n",
    "for ri, (index, row) in enumerate(original_result.iterrows()):\n",
    "    chit = row[\"HITId\"]\n",
    "    if chit in hit_infos:\n",
    "        hit_infos[chit][\"workerid\"].append(row[\"WorkerId\"])\n",
    "    else:\n",
    "        hit_infos[chit] = dict()\n",
    "        hit_infos[chit][\"workerid\"] = [row[\"WorkerId\"]]\n",
    "        hit_infos[chit][\"videoid\"] = [row[\"Input.video{}_id\".format(k+1)] for k in range(5)]\n",
    "        hit_infos[chit][\"questions\"] = [row[\"Input.question{}\".format(k+1)] for k in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_answers = dict() # hit: answers, standard answer, approve_sign\n",
    "\n",
    "with open(\"batch1_review_result.txt\",\"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        if \"HITId:\" in line:\n",
    "            current_hit = line[7:-1]\n",
    "            hit_answers[current_hit] = dict()\n",
    "            hit_answers[current_hit][\"anss\"] = []\n",
    "            hit_answers[current_hit][\"approve\"] = []\n",
    "        elif \"Standard answers:\" in line:\n",
    "            hit_answers[current_hit][\"stdans\"] = eval(line[18:])\n",
    "        elif \"Appr \"in line:\n",
    "            hit_answers[current_hit][\"approve\"].append(True)\n",
    "            hit_answers[current_hit][\"anss\"].append(arrange_answers(eval(line[7:-1])[0]))\n",
    "        elif \"Reje \" in line or \"Midd \" in line:\n",
    "            hit_answers[current_hit][\"approve\"].append(False)\n",
    "            hit_answers[current_hit][\"anss\"].append(arrange_answers(eval(line[7:-1])[0]))\n",
    "\n",
    "with open(\"batch2_review_result.txt\",\"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        if \"HITId:\" in line:\n",
    "            current_hit = line[7:-1]\n",
    "            hit_answers[current_hit] = dict()\n",
    "            hit_answers[current_hit][\"anss\"] = []\n",
    "            hit_answers[current_hit][\"approve\"] = []\n",
    "        elif \"Standard answers:\" in line:\n",
    "            hit_answers[current_hit][\"stdans\"] = eval(line[18:])\n",
    "        elif \"Appr \"in line:\n",
    "            hit_answers[current_hit][\"approve\"].append(True)\n",
    "            hit_answers[current_hit][\"anss\"].append(arrange_answers(eval(line[7:-1])[0]))\n",
    "        elif \"Reje \" in line or \"Midd \" in line:\n",
    "            hit_answers[current_hit][\"approve\"].append(False)\n",
    "            hit_answers[current_hit][\"anss\"].append(arrange_answers(eval(line[7:-1])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValWorkerScores = dict()\n",
    "TestWorkerScores = dict()\n",
    "AllWorkerScores = dict()\n",
    "\n",
    "for (hit,item) in hit_answers.items():\n",
    "    Anss = [[] for _ in range(QuestionNumber)]\n",
    "    for wid in range(len(item[\"approve\"])):  # each worker\n",
    "        if item[\"approve\"][wid]:  # approve\n",
    "            for qid in range(QuestionNumber):\n",
    "                Anss[qid].append(item[\"anss\"][wid][qid])\n",
    "    for qid in range(QuestionNumber):\n",
    "        # filter out unavailable video\n",
    "        if sum([\"unavailable video\" == ite for ite in \n",
    "                [ite2 for ite1 in Anss[qid]\n",
    "                 for ite2 in ite1]]) < 2: # not unavailable video\n",
    "            scores = human_metric(Anss[qid],item[\"stdans\"][qid],RemoveZero=True)\n",
    "            if ((val[\"video_id\"] == hit_infos[hit][\"videoid\"][qid]) &\n",
    "                (val[\"question\"] == hit_infos[hit][\"questions\"][qid])).any():\n",
    "                # validation\n",
    "                sid = 0\n",
    "                for wwi in range(len(item[\"approve\"])):\n",
    "                    if item[\"approve\"][wwi]:\n",
    "                        wwid = hit_infos[hit][\"workerid\"][wwi]\n",
    "                        if wwid in ValWorkerScores:\n",
    "                            ValWorkerScores[wwid].append(scores[sid])\n",
    "                        else:\n",
    "                            ValWorkerScores[wwid] = [scores[sid]]\n",
    "                        sid += 1\n",
    "            elif ((test[\"video_id\"] == hit_infos[hit][\"videoid\"][qid]) &\n",
    "                  (test[\"question\"] == hit_infos[hit][\"questions\"][qid])).any():\n",
    "                # test\n",
    "                sid = 0\n",
    "                for wwi in range(len(item[\"approve\"])):\n",
    "                    if item[\"approve\"][wwi]:\n",
    "                        wwid = hit_infos[hit][\"workerid\"][wwi]\n",
    "                        if wwid in TestWorkerScores:\n",
    "                            TestWorkerScores[wwid].append(scores[sid])\n",
    "                        else:\n",
    "                            TestWorkerScores[wwid] = [scores[sid]]\n",
    "                        sid += 1\n",
    "            else:\n",
    "                raise LookupError(\"Can not find the video question in Val and Test!\")\n",
    "            # whole\n",
    "            sid = 0\n",
    "            for wwi in range(len(item[\"approve\"])):\n",
    "                if item[\"approve\"][wwi]:\n",
    "                    wwid = hit_infos[hit][\"workerid\"][wwi]\n",
    "                    if wwid in AllWorkerScores:\n",
    "                        AllWorkerScores[wwid].append(scores[sid])\n",
    "                    else:\n",
    "                        AllWorkerScores[wwid] = [scores[sid]]\n",
    "                    sid += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average score for each workers\n",
    "AveValWorker = [np.mean(v,axis=0) for v in ValWorkerScores.values()]\n",
    "AveTestWorker = [np.mean(v,axis=0) for v in TestWorkerScores.values()]\n",
    "AveAllWorker = [np.mean(v,axis=0) for v in AllWorkerScores.values()]\n",
    "\n",
    "# average score\n",
    "AveVal = np.mean(AveValWorker,axis=0)\n",
    "AveTest = np.mean(AveTestWorker,axis=0)\n",
    "AveAll = np.mean(AveAllWorker,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75527722, 0.60982668, 0.51348505, 0.48311205, 0.45561596,\n",
       "       0.70895671, 0.557329  , 0.4651634 , 0.43577816, 0.41001618,\n",
       "       0.86313238, 0.76650696, 0.68774768, 0.66167367, 0.63092492,\n",
       "       0.82326183, 0.70957198, 0.62909345, 0.60181429, 0.57145052,\n",
       "       0.7890162 , 0.66617902, 0.58143996, 0.55275951, 0.52392365])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AveTest"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
